version: '3.3'

services:
    sparkresourcemanager:
      container_name: sparkresourcemanager
      hostname: sparkresourcemanager
      image: is8200/sparkresourcemanager:0.1
      environment:
        - JAVA_HOME=${LJAVA_HOME}
        - container=${container}
      privileged: true
      networks:
        spark-network:
          ipv4_address: 172.18.0.4
      deploy:
        resources:
          limits:
            cpus: '0.50'
            memory: 512M
      extra_hosts:
        - "sparkclient:172.18.0.2"
        - "sparknamenode:172.18.0.3"
        - "sparkresourcemanager:172.18.0.4"
        - "sparkdatanode00:172.18.0.5"
        - "sparknodemanager00:172.18.0.6"
      expose:
        - "22"
        - "8020"
      volumes:
        - "e:/kiha/sharedDir/cgroup:/sys/fs/cgroup:ro"
        - "e:/kiha/sharedDir/hadoopConf:/etc/hadoop/conf:ro"
      cap_add:
        - SYS_ADMIN
      depends_on:
        - sparknodemanager00

    sparknodemanager00:
      container_name: sparknodemanager00
      hostname: sparknodemanager00
      image: is8200/sparknodemanager:0.1
      environment:
        - JAVA_HOME=${LJAVA_HOME}
        - container=${container}
      privileged: true
      networks: 
        spark-network:
          ipv4_address: 172.18.0.6
      deploy:
        resources:
          limits:
            cpus: '0.50'
            memory: 512M
      extra_hosts:
        - "sparkclient:172.18.0.2"
        - "sparknamenode:172.18.0.3"
        - "sparkresourcemanager:172.18.0.4"
        - "sparkdatanode00:172.18.0.5"
        - "sparknodemanager00:172.18.0.6" 
      expose:
        - "22"
        - "8020"
      volumes:
        - "e:/kiha/sharedDir/cgroup:/sys/fs/cgroup:ro"
        - "e:/kiha/sharedDir/hadoopConf:/etc/hadoop/conf:ro"
      cap_add:
        - SYS_ADMIN

networks:
  default:
    driver: tempOne-1
  spark-network:
    external:
      name: spark-network
